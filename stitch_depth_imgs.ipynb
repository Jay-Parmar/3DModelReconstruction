{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matchers:\n",
    "    def __init__(self):\n",
    "        # self.surf = cv2.xfeatures2d.SURF_create()\n",
    "        self.sift = cv2.SIFT_create()\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm=1, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    def cylindrical_warp(self, img):\n",
    "        \"\"\" Warp an image onto a cylinder. \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        # Assume K is the camera matrix\n",
    "        f = self.K[0, 0]  # focal length is usually the (1,1) element of the camera matrix\n",
    "\n",
    "        # Define the cylindrical coordinates\n",
    "        cylinder = np.zeros_like(img)\n",
    "        cylinder_x = np.linspace(-w / 2, w / 2, w)\n",
    "        cylinder_y = np.linspace(-h / 2, h / 2, h)\n",
    "        cylinder_x, cylinder_y = np.meshgrid(cylinder_x, cylinder_y)\n",
    "\n",
    "        # Convert (x, y) on the cylinder to (x', y') on the original image\n",
    "        theta = cylinder_x / f\n",
    "        h = cylinder_y / f\n",
    "        x_new = f * np.tan(theta) + w / 2\n",
    "        y_new = f * h / np.cos(theta) + h / 2\n",
    "\n",
    "        # Ensure coordinates are within image bounds\n",
    "        x_new = np.clip(x_new, 0, w - 1)\n",
    "        y_new = np.clip(y_new, 0, h - 1)\n",
    "\n",
    "        # Map the original image onto the cylinder\n",
    "        cylinder = cv2.remap(img, x_new.astype(np.float32), y_new.astype(np.float32), cv2.INTER_LINEAR)\n",
    "        \n",
    "        return cylinder\n",
    "\n",
    "    def match(self, i1, i2, direction=None):\n",
    "        # i1_cyl = self.cylindrical_warp(i1, self.K)\n",
    "        # i2_cyl = self.cylindrical_warp(i2, self.K)\n",
    "        imageSet1 = self.getSIFTFeatures(i1)\n",
    "        imageSet2 = self.getSIFTFeatures(i2)\n",
    "        print(\"Direction : \", direction)\n",
    "        matches = self.flann.knnMatch(\n",
    "            imageSet2['des'],\n",
    "            imageSet1['des'],\n",
    "            k=2\n",
    "            )\n",
    "        good = []\n",
    "        for i , (m, n) in enumerate(matches):\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good.append((m.trainIdx, m.queryIdx))\n",
    "\n",
    "        if len(good) > 4:\n",
    "            pointsCurrent = imageSet2['kp']\n",
    "            pointsPrevious = imageSet1['kp']\n",
    "\n",
    "            matchedPointsCurrent = np.float32(\n",
    "                [pointsCurrent[i].pt for (__, i) in good]\n",
    "            )\n",
    "            matchedPointsPrev = np.float32(\n",
    "                [pointsPrevious[i].pt for (i, __) in good]\n",
    "                )\n",
    "\n",
    "            H, s = cv2.findHomography(matchedPointsCurrent, matchedPointsPrev, cv2.RANSAC, 4)\n",
    "            return H\n",
    "        return None\n",
    "\n",
    "    def getSIFTFeatures(self, im):\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = self.sift.detectAndCompute(gray, None)\n",
    "        return {'kp':kp, 'des':des}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stitch1:\n",
    "    def __init__(self, imgs):\n",
    "        self.images = imgs\n",
    "        self.count = len(self.images)\n",
    "        self.left_list, self.right_list, self.center_im = [], [], None\n",
    "        self.matcher_obj = Matchers()\n",
    "        self.prepare_lists()\n",
    "\n",
    "    def prepare_lists(self):\n",
    "        print(\"Number of images : %d\"%self.count)\n",
    "        self.centerIdx = self.count\n",
    "        # print(\"Center index image : %d\"%self.centerIdx)\n",
    "        # self.center_im = self.images[int(self.centerIdx)]\n",
    "        for i in range(self.count):\n",
    "            if(i<=self.centerIdx):\n",
    "                self.left_list.append(self.images[i])\n",
    "            else:\n",
    "                self.right_list.append(self.images[i])\n",
    "        print(\"Image lists prepared\")\n",
    "\n",
    "    def leftshift(self):\n",
    "        # self.left_list.reverse()\n",
    "        a = self.left_list[0]\n",
    "        try:\n",
    "            for b in self.left_list[1:]:\n",
    "                H = self.matcher_obj.match(a, b, 'left')\n",
    "                # print(\"Homography is : \", H)\n",
    "                if H is None:\n",
    "                    break\n",
    "                xh = np.linalg.inv(H)\n",
    "                # print(\"Inverse Homography :\", xh)\n",
    "                ds = np.dot(xh, np.array([a.shape[1], a.shape[0], 1]))\n",
    "                ds = ds/ds[-1]\n",
    "                # print(\"final ds=>\", ds)\n",
    "                f1 = np.dot(xh, np.array([0,0,1]))\n",
    "                print(\":::f1\", f1)\n",
    "                f1 = f1/f1[-1]\n",
    "\n",
    "                xh[0][-1] += abs(f1[0])\n",
    "                xh[1][-1] += abs(f1[1])\n",
    "                # ds = np.dot(xh, np.array([a.shape[1], a.shape[0], 1]))\n",
    "\n",
    "                offsety = abs(int(f1[1]))\n",
    "                offsetx = abs(int(f1[0]))\n",
    "                required_x = offsetx + b.shape[1]\n",
    "                required_y = offsety + b.shape[0]\n",
    "                dsize = (max(int(ds[0])+offsetx, required_x), max(int(ds[1]) + offsety, required_y))\n",
    "\n",
    "                # dsize = (int(ds[0])+offsetx, int(ds[1]) + offsety)\n",
    "                print(\"image dsize =>\", dsize)\n",
    "                tmp = cv2.warpPerspective(a, xh, dsize)\n",
    "                # cv2.imshow(\"warped\", tmp)\n",
    "                # cv2.waitKey()\n",
    "                # print(\":::Offset X:\", offsetx, \"Offset Y:\", offsety)\n",
    "                # print(\":::Base image shape:\", a.shape)\n",
    "                # print(\":::Transformed image shape:\", tmp.shape)\n",
    "                # print(\":::Sub-image to insert shape:\", b.shape)\n",
    "                # print(\":::Calculated destination slice:\", (offsety, b.shape[0]+offsety), (offsetx, b.shape[1]+offsetx))\n",
    "\n",
    "                tmp[offsety:(b.shape[0]+offsety), offsetx:(b.shape[1]+offsetx)] = b\n",
    "                a = tmp\n",
    "        except Exception:\n",
    "            print(\"Image someeeee failed:::\")\n",
    "            print(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        self.leftImage = a\n",
    "\n",
    "\n",
    "    def rightshift(self):\n",
    "        for each in self.right_list:\n",
    "            H = self.matcher_obj.match(self.leftImage, each, 'right')\n",
    "            print(\"Homography :\", H)\n",
    "            if H is None:\n",
    "                continue\n",
    "            txyz = np.dot(H, np.array([each.shape[1], each.shape[0], 1]))\n",
    "            txyz = txyz/txyz[-1]\n",
    "            dsize = (int(txyz[0])+self.leftImage.shape[1], int(txyz[1])+self.leftImage.shape[0])\n",
    "            tmp = cv2.warpPerspective(each, H, dsize)\n",
    "            # cv2.imshow(\"tp\", tmp)\n",
    "            # cv2.waitKey()\n",
    "            # tmp[:self.leftImage.shape[0], :self.leftImage.shape[1]]=self.leftImage\n",
    "            tmp = self.mix_and_match(self.leftImage, tmp)\n",
    "            print(\"tmp shape\",tmp.shape)\n",
    "            print(\"self.leftimage shape=\", self.leftImage.shape)\n",
    "            self.leftImage = tmp\n",
    "        # self.showImage()\n",
    "\n",
    "\n",
    "\n",
    "    def mix_and_match(self, leftImage, warpedImage):\n",
    "        i1y, i1x = leftImage.shape[:2]\n",
    "        i2y, i2x = warpedImage.shape[:2]\n",
    "        print(leftImage[-1,-1])\n",
    "\n",
    "        t = time.time()\n",
    "        black_l = np.where(leftImage == np.array([0,0,0]))\n",
    "        black_wi = np.where(warpedImage == np.array([0,0,0]))\n",
    "        print(time.time() - t)\n",
    "        print(black_l[-1])\n",
    "\n",
    "        for i in range(0, i1x):\n",
    "            for j in range(0, i1y):\n",
    "                try:\n",
    "                    if(np.array_equal(leftImage[j,i],np.array([0,0,0])) and  np.array_equal(warpedImage[j,i],np.array([0,0,0]))):\n",
    "                        # print \"BLACK\"\n",
    "                        # instead of just putting it with black, \n",
    "                        # take average of all nearby values and avg it.\n",
    "                        warpedImage[j,i] = [0, 0, 0]\n",
    "                    else:\n",
    "                        if(np.array_equal(warpedImage[j,i],[0,0,0])):\n",
    "                            # print \"PIXEL\"\n",
    "                            warpedImage[j,i] = leftImage[j,i]\n",
    "                        else:\n",
    "                            if not np.array_equal(leftImage[j,i], [0,0,0]):\n",
    "                                bw, gw, rw = warpedImage[j,i]\n",
    "                                bl,gl,rl = leftImage[j,i]\n",
    "                                # b = (bl+bw)/2\n",
    "                                # g = (gl+gw)/2\n",
    "                                # r = (rl+rw)/2\n",
    "                                warpedImage[j, i] = [bl,gl,rl]\n",
    "                except:\n",
    "                    pass\n",
    "        # cv2.imshow(\"waRPED mix\", warpedImage)\n",
    "        # cv2.waitKey()\n",
    "        return warpedImage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def trim_left(self):\n",
    "        pass\n",
    "\n",
    "    def showImage(self, string=None):\n",
    "        # if string == 'left':\n",
    "        cv2.imshow(\"left image\", self.leftImage)\n",
    "        #   # cv2.imshow(\"left image\", cv2.resize(self.leftImage, (400,400)))\n",
    "        # elif string == \"right\":\n",
    "        #   cv2.imshow(\"right Image\", self.rightImage)\n",
    "        cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stitcher:\n",
    "\n",
    "    def stitch(self, images, depths, ratio=0.75, reprojThresh=4.0, showMatches=False):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        res = images[0]\n",
    "        depth_res = depths[0]\n",
    "        for img, dep in zip(images[1:], depths[1:]):\n",
    "            print(\"types\", type(img), type(dep), type(res), type(depth_res))\n",
    "\n",
    "            (kpsA, featuresA) = self.detectAndDescribe(res)\n",
    "            (kpsB, featuresB) = self.detectAndDescribe(img)\n",
    "            # match features between the two images\n",
    "            M = self.matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh)\n",
    "            # if the match is None, then there aren't enough matched\n",
    "            # keypoints to create a panorama\n",
    "            if M is None:\n",
    "                break\n",
    "                return None\n",
    "            \n",
    "            # otherwise, apply a perspective warp to stitch the images\n",
    "            # together\n",
    "            (matches, H, status) = M\n",
    "            result = cv2.warpPerspective(\n",
    "                img, H, (img.shape[1] + res.shape[1], max(img.shape[0], res.shape[0])),\n",
    "                cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0)\n",
    "            )\n",
    "            depth_result = cv2.warpPerspective(\n",
    "                dep, H, (dep.shape[1] + depth_res.shape[1], max(dep.shape[0], depth_res.shape[0])),\n",
    "                cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0)\n",
    "            )\n",
    "            print(res.shape, result.shape, dep.shape, depth_result.shape)\n",
    "            result[0:res.shape[0], 0:res.shape[1]] = res\n",
    "            depth_result[0:depth_res.shape[0], 0:depth_res.shape[1]] = depth_res\n",
    "            # check to see if the keypoint matches should be visualized\n",
    "            result = self.remove_black_borders(result)\n",
    "            depth_result = self.remove_black_borders(depth_result)\n",
    "            # print(\":::remove borders\", type(result))\n",
    "            if showMatches:\n",
    "                vis = self.drawMatches(img, res, kpsA, kpsB, matches, status)\n",
    "                # return a tuple of the stitched image and the\n",
    "                # visualization\n",
    "                cv2.imshow(\"VIZZZZZ\", result)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "                return (result, vis)\n",
    "            res = result\n",
    "            depth_res = depth_result\n",
    "            # return the stitched image\n",
    "            # cv2.imshow(\"Result\", result)\n",
    "            # cv2.waitKey(0)\n",
    "            # return result\n",
    "        return res, depth_res\n",
    "    \n",
    "    def remove_black_borders(self, img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, binary = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding rectangle for the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        cropped_image = img[y:y+h, x:x+w]\n",
    "        \n",
    "        return cropped_image\n",
    "\n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        # cv2.imshow(\"DETECTTT\", image)\n",
    "        # cv2.waitKey(0)\n",
    "        print(\":::typeeeee\", type(image))\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect keypoints in the image\n",
    "        detector = cv2.SIFT_create()\n",
    "        kps = detector.detect(gray)\n",
    "        # extract features from the image\n",
    "        (kps, features) = detector.compute(gray, kps)\n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "    \n",
    "\n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        # matcher = cv2.cuda.DescriptorMatcher_createBFMatcher(cv2.NORM_HAMMING)\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "        \n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):\n",
    "        # initialize the output visualization image\n",
    "        (hA, wA) = imageA.shape[:2]\n",
    "        (hB, wB) = imageB.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imageA\n",
    "        vis[0:hB, wA:] = imageB\n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "                ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    "        # return the visualization\n",
    "        return vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    imgs = []\n",
    "    for img in sorted(glob.glob(path), key=os.path.getctime):\n",
    "        # n = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY)\n",
    "        n = cv2.imread(img)\n",
    "        # n = imutils.resize(n, width=500)\n",
    "        n = cv2.pyrDown(n)\n",
    "\n",
    "        # Uncomment below lines based on the orientation of the camera\n",
    "        imgs.append(np.rot90(n, k=3))\n",
    "        # imgs.append(n)\n",
    "    print(\"num_images\", len(imgs))\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_images 19\n",
      "num_images 19\n"
     ]
    }
   ],
   "source": [
    "# img7, img8 = cv2.imread('.\\Imgs\\good\\\\0_test\\img_0.jpeg'), cv2.imread('.\\Imgs\\good\\\\0_test\\img_0.jpeg')\n",
    "imgs = get_images('.\\Imgs\\good\\\\0_test\\*')\n",
    "depths = get_images('.\\Imgs\\good\\\\depth\\*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher_cv = cv2.Stitcher.create()\n",
    "status1, pano_img = stitcher_cv.stitch(imgs)\n",
    "status2, pano_dep = stitcher_cv.stitch(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 600, 3) (960, 1200, 3) (960, 600, 3) (960, 1200, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(602, 601, 3) (960, 1201, 3) (960, 600, 3) (960, 1201, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(787, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(913, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(913, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(913, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(913, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1202, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1200, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1303, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1303, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1303, 3)\n",
      "types <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      ":::typeeeee <class 'numpy.ndarray'>\n",
      "(960, 602, 3) (960, 1202, 3) (960, 600, 3) (960, 1303, 3)\n"
     ]
    }
   ],
   "source": [
    "st = Stitcher()\n",
    "\n",
    "yo, yo_depth = st.stitch(imgs,depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpano_dep\u001b[49m\u001b[43m,\u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Demo\\anaconda3\\envs\\pc_merge\\lib\\site-packages\\matplotlib\\pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   3563\u001b[0m         X,\n\u001b[0;32m   3564\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3565\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3566\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   3567\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   3568\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3569\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3570\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3571\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   3572\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   3573\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   3574\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   3575\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m   3576\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   3577\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   3578\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3579\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3580\u001b[0m     )\n\u001b[0;32m   3581\u001b[0m     sci(__ret)\n\u001b[0;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Demo\\anaconda3\\envs\\pc_merge\\lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1474\u001b[0m             ax,\n\u001b[0;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Demo\\anaconda3\\envs\\pc_merge\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Demo\\anaconda3\\envs\\pc_merge\\lib\\site-packages\\matplotlib\\image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Demo\\anaconda3\\envs\\pc_merge\\lib\\site-packages\\matplotlib\\image.py:692\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    690\u001b[0m A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    693\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pano_dep,aspect=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 602, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "for file in glob.glob(\".\\Imgs\\good\\\\depth\\*\"):\n",
    "    src_im = Image.open(file)\n",
    "    src_im.rotate(90).save(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
